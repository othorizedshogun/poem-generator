{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing and importing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq git+https://github.com/huggingface/transformers.git\n",
    "%pip install -qq git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.8/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/python/3.10.8/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.8/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up base model and adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = \"./model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the LoRA model i.e base model along with the adapter\n",
    "inference_model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=2048, out_features=2048, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear(\n",
       "                in_features=2048, out_features=256, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the model's generate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [Form: imagery, Topic: identity]\n",
      "Poem:\n",
      "When I was a little girl,\n",
      "I used to play with my dolls.\n",
      "They were so realistic\n",
      "That I couldn't help but fall in love\n",
      "With them, even though they were made of plastic\n",
      "And I had to put them back in the box\n",
      "As soon as I got tired of playing with them\n",
      "But I loved them so much that I never let them go\n",
      "Because I wanted them to be real\n",
      "So when I grew up, I took them with me\n",
      "To my grandmother's house\n",
      "She had a dollhouse\n",
      "In the corner of the living room\n",
      "It was just like the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: [Form: imagery, Topic: identity]\n",
    "Poem:\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, num_beams=10, no_repeat_ngram_size=2, length_penalty=0.8, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001000001001101 Input: [Form: imagery, Topic: identity]\n",
      "Poem:\n",
      "I'm not who you think I am\n",
      "And I'll never be what you want me to be\n",
      "Because I don't want you to think me that way\n",
      "You're the one who's trying to convince me\n",
      "That I should be like you\n",
      "But you've got the wrong end of the stick\n",
      "'Cause if I was that person you thought I would be,\n",
      "There'd be no need for this to happen\n",
      "So why are you so determined to make me feel like this?\n",
      "Why do I have to feel so bad about myself\n",
      "When I know that I deserve so much more than this\n",
      "If you really loved me, why would you try to ruin my life\n",
      "Just so that you could be the center of my world? Input\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"1001000001001101 Input: [Form: imagery, Topic: identity]\n",
    "Poem:\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=200, num_beams=10, no_repeat_ngram_size=2, length_penalty=0.8, early_stopping=True)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_poem(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=200, num_beams=10, no_repeat_ngram_size=2, length_penalty=0.8, early_stopping=True, temperature=0.61)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10825989035610277917"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secrets.randbits(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.8/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.61` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [Form: sonnet, Topic: identity]\n",
      "Poem:\n",
      "Sonnet 147\n",
      "I am a man, and not a woman,\n",
      "But I am not what I pretend\n",
      "To be, nor am I what you think\n",
      "That I should be.\n",
      "There is no such thing as manhood\n",
      "Or womanhood, but there is\n",
      "A man's mind and a women'\n",
      "His body and his soul;\n",
      "And there are men and women\n",
      "Of both sexes, of both genders\n",
      "Both masculine and feminine, both\n",
      "Male and female, neither male\n",
      "Nor female; nor is there any\n",
      "Difference between the male and the\n",
      "female, for they are the same\n",
      "In essence and in substance\n",
      "As man and woman are in all\n",
      "Their essences and all their\n",
      "Substances; and, therefore, I\n",
      "Am neither man nor woman.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: [Form: sonnet, Topic: identity]\n",
    "Poem:\n",
    "\"\"\"\n",
    "create_poem(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [Topic: god]\n",
      "Poem:\n",
      "God, I'm sorry,\n",
      "I don't know what to say.\n",
      "You've been there for me\n",
      "When I needed you the most\n",
      "And now you're gone\n",
      "It's hard to believe\n",
      "That you were my best friend\n",
      "But I know you are\n",
      "Without you I would be lost\n",
      "All I can do is pray\n",
      "For you to come back\n",
      "So that we can spend more time\n",
      "Talking and laughing\n",
      "Laughing and crying\n",
      "Just like we did when we were kids\n",
      "Thank you for all you have done\n",
      "To make me the person I am today\n",
      "A person who loves you\n",
      "More than anything else in the world\n",
      "Because I miss you so much\n",
      "Please forgive me for not being able to tell you how much I love you and how sorry I feel for the pain I have caused you in my life\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: [Topic: god]\n",
    "Poem:\n",
    "\"\"\"\n",
    "create_poem(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "8388.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accelerate @ git+https://github.com/huggingface/accelerate.git@55747318a0f47cdfbc281e11269ba96214e4092d\n",
      "anyio==4.0.0\n",
      "argon2-cffi==23.1.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.3.0\n",
      "asttokens==2.4.0\n",
      "async-lru==2.0.4\n",
      "attrs==23.1.0\n",
      "Babel==2.13.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "bitsandbytes==0.41.1\n",
      "bleach==6.1.0\n",
      "certifi==2023.7.22\n",
      "cffi==1.16.0\n",
      "charset-normalizer==3.3.0\n",
      "colorama==0.4.6\n",
      "comm==0.1.4\n",
      "contourpy==1.1.1\n",
      "cycler==0.12.1\n",
      "debugpy==1.8.0\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "exceptiongroup==1.1.3\n",
      "executing==2.0.0\n",
      "fastjsonschema==2.18.1\n",
      "filelock==3.12.4\n",
      "fonttools==4.43.1\n",
      "fqdn==1.5.1\n",
      "fsspec==2023.9.2\n",
      "gitdb==4.0.10\n",
      "GitPython==3.1.38\n",
      "huggingface-hub==0.17.3\n",
      "idna==3.4\n",
      "ipykernel==6.25.2\n",
      "ipython==8.16.1\n",
      "isoduration==20.11.0\n",
      "jedi==0.19.1\n",
      "Jinja2==3.1.2\n",
      "joblib==1.3.2\n",
      "json5==0.9.14\n",
      "jsonpointer==2.4\n",
      "jsonschema==4.19.1\n",
      "jsonschema-specifications==2023.7.1\n",
      "jupyter-events==0.8.0\n",
      "jupyter-lsp==2.2.0\n",
      "jupyter-server-mathjax==0.2.6\n",
      "jupyter_client==8.4.0\n",
      "jupyter_core==5.4.0\n",
      "jupyter_server==2.8.0\n",
      "jupyter_server_terminals==0.4.4\n",
      "jupyterlab==4.0.7\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab_git==0.43.0\n",
      "jupyterlab_server==2.25.0\n",
      "kiwisolver==1.4.5\n",
      "MarkupSafe==2.1.3\n",
      "matplotlib==3.8.0\n",
      "matplotlib-inline==0.1.6\n",
      "mistune==3.0.2\n",
      "mpmath==1.3.0\n",
      "nbclient==0.8.0\n",
      "nbconvert==7.9.2\n",
      "nbdime==3.2.1\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.5.8\n",
      "networkx==3.1\n",
      "notebook_shim==0.2.3\n",
      "numpy==1.26.1\n",
      "nvidia-cublas-cu12==12.1.3.1\n",
      "nvidia-cuda-cupti-cu12==12.1.105\n",
      "nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "nvidia-cuda-runtime-cu12==12.1.105\n",
      "nvidia-cudnn-cu12==8.9.2.26\n",
      "nvidia-cufft-cu12==11.0.2.54\n",
      "nvidia-curand-cu12==10.3.2.106\n",
      "nvidia-cusolver-cu12==11.4.5.107\n",
      "nvidia-cusparse-cu12==12.1.0.106\n",
      "nvidia-nccl-cu12==2.18.1\n",
      "nvidia-nvjitlink-cu12==12.2.140\n",
      "nvidia-nvtx-cu12==12.1.105\n",
      "overrides==7.4.0\n",
      "packaging==23.2\n",
      "pandas==2.1.1\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "peft @ git+https://github.com/huggingface/peft.git@884b1ac3a8ef49c9301b5bbf02e8bc64349e95f9\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "Pillow==10.1.0\n",
      "platformdirs==3.11.0\n",
      "plotly==5.17.0\n",
      "prometheus-client==0.17.1\n",
      "prompt-toolkit==3.0.39\n",
      "psutil==5.9.6\n",
      "ptyprocess==0.7.0\n",
      "pure-eval==0.2.2\n",
      "pycparser==2.21\n",
      "Pygments==2.16.1\n",
      "pyparsing==3.1.1\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "pytz==2023.3.post1\n",
      "PyYAML==6.0.1\n",
      "pyzmq==25.1.1\n",
      "referencing==0.30.2\n",
      "regex==2023.10.3\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rpds-py==0.10.6\n",
      "safetensors==0.4.0\n",
      "scikit-learn==1.3.1\n",
      "scipy==1.11.3\n",
      "seaborn==0.13.0\n",
      "Send2Trash==1.8.2\n",
      "six==1.16.0\n",
      "smmap==5.0.1\n",
      "sniffio==1.3.0\n",
      "soupsieve==2.5\n",
      "stack-data==0.6.3\n",
      "sympy==1.12\n",
      "tenacity==8.2.3\n",
      "terminado==0.17.1\n",
      "threadpoolctl==3.2.0\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.14.1\n",
      "tomli==2.0.1\n",
      "torch==2.1.0\n",
      "tornado==6.3.3\n",
      "tqdm==4.66.1\n",
      "traitlets==5.11.2\n",
      "transformers @ git+https://github.com/huggingface/transformers.git@8211c59b9a8fe84d2861446b26542f89a0260e64\n",
      "triton==2.1.0\n",
      "types-python-dateutil==2.8.19.14\n",
      "typing_extensions==4.8.0\n",
      "tzdata==2023.3\n",
      "uri-template==1.3.0\n",
      "urllib3==2.0.7\n",
      "wcwidth==0.2.8\n",
      "webcolors==1.13\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
